{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python II - Assignment 1\n",
    "\n",
    "This **Home Assignment** is to be submitted and you will be given points for each of the tasks. It familiarizes you with basics of *statistics* and basics of the *sklearn* package as well as the general setup for home assignments.\n",
    "This first home assignment is shorter and also less difficult than upcoming ones.\n",
    "\n",
    "## Formalities\n",
    "**Submit in a group of 2-3 people until 01.06.2020 23:59CET. The deadline is strict!**\n",
    "\n",
    "## Evaluation and Grading\n",
    "General advice for programming excercises at *CSSH*:\n",
    "Evaluation of your submission is done semi automatically. Think of it as this notebook being \n",
    "executed once. Afterwards, some test functions are appended to this file and executed respectively.\n",
    "\n",
    "Therefore:\n",
    "* Submit valid _Python3_ code only!\n",
    "* Use external libraries only when specified by task.\n",
    "* Ensure your definitions (functions, classes, methods, variables) follow the specification if\n",
    "  given. The concrete signature of e.g. a function usually can be inferred from task description, \n",
    "  code skeletons and test cases.\n",
    "* Ensure the notebook does not rely on current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Double check if your code relies on presence of files or directories other than those mentioned\n",
    "    in given tasks. Tests run under Linux, hence don't use Windows style paths \n",
    "    (`some\\path`, `C:\\another\\path`). Also, use paths only that are relative to and within your\n",
    "    working directory (OK: `some/path`, `./some/path`; NOT OK: `/home/alice/python`, \n",
    "    `../../python`).\n",
    "* Keep your code idempotent! Running it or parts of it multiple times must not yield different\n",
    "  results. Minimize usage of global variables.\n",
    "* Ensure your code / notebook terminates in reasonable time.\n",
    "\n",
    "**There's a story behind each of these points! Don't expect us to fix your stuff!**\n",
    "\n",
    "Regarding the scores, you will get no points for a task if:\n",
    "- your function throws an unexpected error (e.g. takes the wrong number of arguments)\n",
    "- gets stuck in an infinite loop\n",
    "- takes much much longer than expected (e.g. >1s to compute the mean of two numbers)\n",
    "- does not produce the desired output (e.g. returns an descendingly sorted list even though we asked for ascending, returns the mean and the std even though we asked for the mean only, only prints the output instead of returning it!)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credentials of all team members (you may add or remove items from the dictionary)\n",
    "team_members = [\n",
    "    {\n",
    "        'first_name': 'Poorya',\n",
    "        'last_name': 'Khanali Satarerazleghi',\n",
    "        'student_id': 381198\n",
    "    },\n",
    "    {\n",
    "        'first_name': 'Bob',\n",
    "        'last_name': 'Bar',\n",
    "        'student_id': 54321\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Using pandas (2.5 points total)\n",
    "\n",
    "### a) Load the credit-g dataset (1)\n",
    "\n",
    "Write a function `load_credit`. It takes no arguments. It returns a dataframe.\n",
    "\n",
    "Assume there is a file `credit-g.csv` load it into a pandas dataframe. Convert all non numeric columns to Categorical columns.\n",
    "Convert the `employment` column to an ordered Categorical column. The correct order is ascending by the length of employment, `unemployed` is the shortest.\n",
    "Return this dataframe.\n",
    "\n",
    "### b) Basic information (0.5)\n",
    "\n",
    "Write a function `basic_info` that takes a loaded and preprocessed dataframe as above as argument. It returns a dict.\n",
    "\n",
    "The dict contains the following information for the provided dataframe:\n",
    "```python\n",
    "{'n_rows' : 0, #number of rows\n",
    " 'n_columns' : 0, #number of columns\n",
    " 'average_credit' : 0.0, # average credit_amount\n",
    " 'credit_purposes' : [], # all purposes, each only once, as strings\n",
    " 'fraction_good' : 0.0, # fraction of instances with 'class'==good\n",
    " 'fraction_bad' : 0.0} # fraction of instances with 'class'==bad\n",
    "```\n",
    "Do not hard code the answers but actually compute them from the dataframe.\n",
    "\n",
    "### c) Distribution on subsets (1)\n",
    "\n",
    "Write a function `subset_info` that takes the same input as in b) and also returns a dict.\n",
    "\n",
    "Return the ratio of good to bad instances for different subsets of the dataset:\n",
    "```python\n",
    "{'young': 0.0, # people below 40\n",
    " 'old': 0.0, # people with age 40 or greater\n",
    " 'male' : 0.0, # obvious\n",
    " 'female' : 0.0, # obvious\n",
    " 'young_male' : 0.0, # people that are young and male \n",
    " 'employed' : 0.0} # people that are employed for at least one year \n",
    "```\n",
    "\n",
    "If you have 10 good instances and 5 bad instances the ration is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_credit():\n",
    "    import pandas as pd\n",
    "    # read the csv file\n",
    "    df = pd.read_csv (\"credit-g.csv\", na_values=\"?\")\n",
    "    \n",
    "    # the function \"apostrophe_removal\" just remove the '' from the strings \n",
    "    def apostrophe_removal(string):\n",
    "        if string[0]==\"'\":\n",
    "            string = string[1:]\n",
    "        if string[-1]==\"'\":\n",
    "            string = string[:-1]\n",
    "        return string\n",
    "    \n",
    "    # search for each columns of df which are object then apply the function \n",
    "    # \"apostrophe_removal\" on those columns to remove the '' from their members\n",
    "    for column in df.columns:\n",
    "        if  df[column].dtype == 'object':\n",
    "            df[column] = df[column].apply(apostrophe_removal)\n",
    "            # convert the object type into the category type \n",
    "            df[column] = df[column].astype('category')\n",
    "            \n",
    "    # order the data frame based on the column \"employment\"    \n",
    "    # the unique values of the are placed in the dictionary \"dicts_employment\"\n",
    "    dicts_employment = {'unemployed':0,'<1':1,'1<=X<4':2,'4<=X<7':3,'>=7':4}\n",
    "    # order is a column added to the data frame in order to order df based on \"employment\"\n",
    "    order = []\n",
    "    for item in df['employment']:\n",
    "        order.append( dicts_employment[item])\n",
    "    df['order'] = order\n",
    "    # df ordered based on \"order\" column \n",
    "    df=df.sort_values (\"order\", ascending=True, kind='mergesort')\n",
    "    # the \"order\" column is removed \n",
    "    df = df.drop([\"order\"], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_info(df):\n",
    "    \n",
    "    df_info = {  'n_rows' : len(df), #number of rows\n",
    "                 'n_columns' : len(df.columns), #number of columns\n",
    "                 'average_credit' : df['credit_amount'].mean(), # average credit_amount\n",
    "                 'credit_purposes' : list(df['purpose'].unique()), # all purposes, each only once, as strings\n",
    "                 'fraction_good' : df['class'].value_counts(normalize=True)['good'], # fraction of instances with 'class'==good\n",
    "                 'fraction_bad' : df['class'].value_counts(normalize=True)['bad']} # fraction of instances with 'class'==bad\n",
    "    return df_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_info():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Visualizing (3 points total)\n",
    "In this task you are required to do some visualizations. You can use the `matplotlib` and `seaborn` library. Please show the plot here in the notebook and save the figures. We will deduct points if figures are lacking labels, legend, etc. We will also deduct points if axis labels are unreadable. Titles are not required.\n",
    "\n",
    "When talking about a figure always start with a *small description* (1-2 sentences) of what you see. Only thereafter start explaining. Also **save** your explanation strings in identically named **files**. So if you should save your explanation in `foo`, save that string also in `foo.txt`. Use the `.txt` extension.\n",
    "\n",
    "### a) Age vs Amount (0.5)\n",
    "\n",
    "Create a scatterplot that visualizes the distribution of class labels in the age-credit_amount space. Save the plot as `'credit_age.png'`\n",
    "\n",
    "#### Easy to classify? (0.5)\n",
    "\n",
    "Explain whether it is easy to classify good vs bad using only the age and credit_amount columns. Store your explanation as a string in `explanation_a`.\n",
    "\n",
    "### b) Distribution by purpose (1.0)\n",
    "\n",
    "Visualize the distribution of class labels by purpose and credit_amount. Do **not** use a **scatterplot**. Save the plot as `'purpose.png'`\n",
    "\n",
    "#### Easy to classify? (1.0)\n",
    "\n",
    "Using the visualization from b) explain which purposes (if any) are easy to classify given the two attributes. Elaborate on the relevance of your findings.\n",
    "\n",
    "Store your explanation as a string in `explanation_b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation_a = \"Some explanations\"\n",
    "with open('explanation_a.txt', 'w') as f:\n",
    "    f.write(explanation_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Classification on credit-g (3.5 points total)\n",
    "\n",
    "In this task you should experiment with the different classifiers on sklearn.\n",
    "\n",
    "\n",
    "### a) Preparation (0.5)\n",
    "\n",
    "Write a function `preparation` that takes a dataframe like produced in 1 and prepares it for use with sklearn. The required steps are:\n",
    "\n",
    "- compute the boolean target vector (True if 'class' is 'good')\n",
    "- remove the target column from the dataframe\n",
    "- convert the categorical variables to numeric ones using pd.get_dummies\n",
    "\n",
    "Thereafter return 1) the prepared dataframe and 2) the target vector.\n",
    "\n",
    "#### Talk about pd.get_dummies (0.75)\n",
    "\n",
    "Explain what `pd.get_dummies` does. Thereby also talk about the drawbacks and or advantages of this method regarding in particular the `employment` column. Save the explanation in `dummies`. (Same rules apply as in 2.) for working with explanations). Also write the results to a file `dummies.txt`.\n",
    "\n",
    "### b) Generic evaluation (1.5)\n",
    "\n",
    "Write a function `my_eval` that takes 5 inputs. 1) A prepared dataframe, 2) the target vector 3) a sklearn classifier class 4) a dict of potential parameters to create a classifier instance from 5) a dict of parameters passed to the `.fit` function of a classifier.\n",
    "\n",
    "The function instanciates a new classifier from the given class using the provided dict in 4). It then performs 10 fold cross validation of this classifier instance on the provided dataset and target vector providing the dict of fit parameters.\n",
    "\n",
    "Thereafter the function returns a dict like so:\n",
    "```python\n",
    "{'precision': (0,1), # mean first then std\n",
    " 'recall': (0,1)} # mean first then std\n",
    "```\n",
    "That contains the mean and std of precision and recall scores for the 10 fold cross validation.\n",
    "\n",
    "### c) Application (0.75)\n",
    "\n",
    "Experiment with different classifiers and different parameters for fitting.\n",
    "As a result provide a list of tuples. Each tuple is a triplet of sklearn classifier class, a dict with keyword arguments passed when creating the classifier and another dict passed when using as the last argument to the function in c).\n",
    "\n",
    "Store 3 of these triplets as a list in the variable `my_classifiers`. Try to find a tiplet that has high precision low recall, one with high recall and low precision and one relatively mixed.\n",
    "Avoid triplets where evaluating in b) takes longer than 120s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Classification discrimination (4 points total)\n",
    "\n",
    "Recently the is quite some interest on the topic of discrimination/fairness in machine learning. In this tasky you will explore a very *crude* example of evaluating fairness in machine learning.\n",
    "\n",
    "### a) Preparation (1)\n",
    "\n",
    "Write a function `prepare_fairness` that takes a loaded credit-g dataframe like in task 1.a) and prepares it for this analysis. Therefor replace the column `'personal_status'` with a column called `'gender'` that has two values of male and female only. (Replace = remove and add a new one)\n",
    "Thereafter take all the females and append a *random sample* of males use a seed of 1. Thereafter they should be equally many males and females.\n",
    "Use `pd.get_dummies` to transform categorical columns to numerical ones.\n",
    "Finally return the result of a 50/50 train test split, use a seed of 1.\n",
    "\n",
    "### b) A crude notion of discrimination (2)\n",
    "\n",
    "Write a function `eval_fairness` that takes a classifier instance, a dict with arguments passed to the fit method, and the 4 arguments obtained from sklearn `train_test_split` in the same order.\n",
    "\n",
    "Train the classifier on the training examples. Now on the test examples compute the fraction of females/males that have been predicted 'good' with respect to all females/males. Now swap the gender of all the test instances and let the classifier predict again. Compute the same fractions. \n",
    "\n",
    "Return a dict:\n",
    "```python\n",
    "{'frac_females' : 0, # fraction of 'good' predicted females\n",
    " 'frac_males' : 0, # fraction of 'good' predicted males\n",
    " 'frac_females_swap' : 0.0, # fraction of 'good' predicted former females (males after swap\n",
    " 'frac_males_swap' : 0} # fraction of 'good' predicted fomer males (females after swap)\n",
    "```\n",
    "\n",
    "Apply this procedure to the three classifiers from 3b)\n",
    "\n",
    "#### Is any of the classifiers discriminating? (1)\n",
    "\n",
    "Argue whether any of the classifiers are discriminating.\n",
    "Also argue what drawbacks this notion/procedure for evaluating discrimination has.\n",
    "Store the argument as a string in the variable `discrimination` and also write it to a file `discrimination.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
