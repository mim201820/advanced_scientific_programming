{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python II - Assignment 3\n",
    "This **Home Assignment** is to be submitted and you will be given points for each of the tasks. The first task will recap a lot of the things you learned in this couse (web scraping, regex, pandas, nltk). The second task focuses primarly on graphs.\n",
    "\n",
    "## Formalities\n",
    "**Submit in a group of 2-3 people until 06.07.2020 23:59CET. The deadline is strict!**\n",
    "\n",
    "## Evaluation and Grading\n",
    "General advice for programming excercises at *CSSH*:\n",
    "Evaluation of your submission is done semi automatically. Think of it as this notebook being \n",
    "executed once. Afterwards, some test functions are appended to this file and executed respectively.\n",
    "\n",
    "Therefore:\n",
    "* Submit valid _Python3_ code only!\n",
    "* We run python 3.8 on our machines, so you can use newer features\n",
    "* Use external libraries only when specified by task.\n",
    "* Ensure your definitions (functions, classes, methods, variables) follow the specification if\n",
    "  given. The concrete signature of e.g. a function usually can be inferred from task description, \n",
    "  code skeletons and test cases.\n",
    "* Ensure the notebook does not rely on current notebook or system state!\n",
    "  * Use `Kernel --> Restart & Run All` to see if you are using any definitions, variables etc. that \n",
    "    are not in scope anymore.\n",
    "  * Double check if your code relies on presence of files or directories other than those mentioned\n",
    "    in given tasks. Tests run under Linux, hence don't use Windows style paths \n",
    "    (`some\\path`, `C:\\another\\path`). Also, use paths only that are relative to and within your\n",
    "    working directory (OK: `some/path`, `./some/path`; NOT OK: `/home/alice/python`, \n",
    "    `../../python`).\n",
    "* Keep your code idempotent! Running it or parts of it multiple times must not yield different\n",
    "  results. Minimize usage of global variables.\n",
    "* Ensure your code / notebook terminates in reasonable time.\n",
    "\n",
    "**There's a story behind each of these points! Don't expect us to fix your stuff!**\n",
    "\n",
    "Regarding the scores, you will get no points for a task if:\n",
    "- your function throws an unexpected error (e.g. takes the wrong number of arguments)\n",
    "- gets stuck in an infinite loop\n",
    "- takes much much longer than expected (e.g. >1s to compute the mean of two numbers)\n",
    "- does not produce the desired output (e.g. returns an descendingly sorted list even though we asked for ascending, returns the mean and the std even though we asked for the mean only, only prints the output instead of returning it!)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Sentiment of Tweets (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Load dataset from the internet (2.0)\n",
    "Consider the twitter data provided in https://github.com/zfz/twitter_corpus/blob/master/full-corpus.csv.\n",
    "\n",
    "Write a function `load_dataset()` that gets the dataset from the internet and loads it into a dataframe with two columns `'gt_sentiment'` and `'original_tweet'` that stores the sentiment and (unprocessed) tweet content from that dataset. It returns that dataframe. Use the requests library to fetch the csv file from the internet. Thereby drop all tweets with \"irrelevant\" sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "def load_dataset():\n",
    "    # the url of the raw CSV \n",
    "    CSV_URL = \"https://raw.githubusercontent.com/zfz/twitter_corpus/master/full-corpus.csv\"\n",
    "    page=requests.get(CSV_URL).content\n",
    "    df=pd.read_csv(StringIO(page.decode('utf-8')))\n",
    "\n",
    "    df = df.rename(columns={'Sentiment': 'gt_sentiment', 'TweetText': 'original_tweet'})\n",
    "    data = df[['gt_sentiment','original_tweet']]\n",
    "    # remove irrelevant tweets\n",
    "    data = data[data['gt_sentiment']!='irrelevant']\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# CSV_URL = \"https://raw.githubusercontent.com/zfz/twitter_corpus/master/full-corpus.csv\"\n",
    "# with requests.Session() as s:\n",
    "#     download = s.get(CSV_URL)\n",
    "\n",
    "#     decoded_content = download.content.decode('utf-8')\n",
    "\n",
    "#     cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
    "#     my_list = list(cr)\n",
    "\n",
    "# pd.DataFrame(my_list[1:],columns=my_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /zfz/twitter_corpus/master/full-corpus.csv (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024CF3605D08>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m             conn = connection.create_connection(\n\u001b[1;32m--> 157\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m             )\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\urllib3\\util\\connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m         \u001b[0maf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m             )\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    993\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"sock\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;31m# Add certificate verification\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m             raise NewConnectionError(\n\u001b[1;32m--> 169\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m             )\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x0000024CF3605D08>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    719\u001b[0m             retries = retries.increment(\n\u001b[1;32m--> 720\u001b[1;33m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m             )\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    435\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /zfz/twitter_corpus/master/full-corpus.csv (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024CF3605D08>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-df2d9b00dc12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# example dataframe:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#       gt_sentiment    original_tweet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#0      positive        Now all @Apple has to do is get swype on the i...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-2c41aafee60d>\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# the url of the raw CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mCSV_URL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://raw.githubusercontent.com/zfz/twitter_corpus/master/full-corpus.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mpage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCSV_URL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    528\u001b[0m         }\n\u001b[0;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myTorch\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    514\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /zfz/twitter_corpus/master/full-corpus.csv (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x0000024CF3605D08>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "df = load_dataset()\n",
    "df.head()\n",
    "# example dataframe:\n",
    "# \tgt_sentiment \toriginal_tweet\n",
    "#0 \tpositive \tNow all @Apple has to do is get swype on the i...\n",
    "#1 \tpositive \t@Apple will be adding more carrier support to ...\n",
    "#2 \tpositive \tHilarious @youtube video - guy does a duet wit...\n",
    "#3 \tpositive \t@RIM you made it too easy for me to switch to ...\n",
    "#4 \tpositive \tI just realized that the reason I got into twi..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Clean tweets (1.5 +0.5)\n",
    "Write a function `clean_tweet(tweet)` that takes a tweet (as string) as input and removes usernames, hashtags and links. Additionally provide a function `clean_all_tweets(df)` that takes a dataframe like above and applies the `clean_tweet` function to all of them. It returns the same dataframe with now an additional column `cleaned_tweet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "\n",
    "    user_handles = re.findall('@\\S+',tweet)\n",
    "    cleaned_tweet = re.sub('@\\S+','',tweet)\n",
    "    \n",
    "    hashtags = re.findall('#\\S+',tweet)\n",
    "    cleaned_tweet = re.sub('#\\S+','',cleaned_tweet)\n",
    "    \n",
    "    # links can be started by \"http://\" or https:// so we have to remove both of them\n",
    "    links1 = re.findall('http://\\S+',tweet)\n",
    "    cleaned_tweet = re.sub('http://\\S+','',cleaned_tweet)\n",
    "\n",
    "    links2 = re.findall('https://\\S+',tweet)\n",
    "    cleaned_tweet = re.sub('https://\\S+','',cleaned_tweet)\n",
    "    # concatenate link1 & link2\n",
    "    links = links1 + links2\n",
    "\n",
    "    \n",
    "    \n",
    "    return cleaned_tweet.strip(), user_handles, hashtags, links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1=\"Digital X Worldwide | Today Is Steve Jobs Day In California @apple http://t.co/QSCHuMIN\"\n",
    "example2=\"Wow. Great deals on refurbed #iPad (first gen) models. RT: Apple offers great deals on refurbished 1st-gen iPads http://t.co/ukWOKBGd @Apple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clean_tweet(example1))\n",
    "# ('Digital X Worldwide | Today Is Steve Jobs Day In California', ['@apple'], [], ['http://t.co/QSCHuMIN'])\n",
    "print(clean_tweet(example2))\n",
    "# ('Wow. Great deals on refurbed  (first gen) models. RT: Apple offers great deals on refurbished 1st-gen iPads', ['@Apple'], ['#iPad'], ['http://t.co/ukWOKBGd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_tweets(df):\n",
    "    df['cleaned_tweet'] = df['original_tweet'].apply(lambda x :clean_tweet(x)[0])\n",
    "    return df\n",
    "clean_all_tweets(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Predict the sentiment (1.5)\n",
    "Write a function `calculate_sentiment(df)` that calculate the sentiment of the tweet using the vader sentiment classifier from nltk. Use the **compound** score of the `polarity_scores`. It is between -1 and 1. You can classify the sentiment as - \n",
    "\n",
    "```\n",
    "[-1,-0.2) -> negative\n",
    "\n",
    "[-0.2,0.2] -> neutral\n",
    "\n",
    "(0.2,1.0] -> positive```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def calculate_sentiment(df):\n",
    "    sia_polarity = SentimentIntensityAnalyzer().polarity_scores\n",
    "    df['sentiment'] = df['cleaned_tweet'].apply(lambda x : sia_polarity(x)['compound'])\n",
    "\n",
    "    def check_sentiment(x):\n",
    "        if  x < -.2:\n",
    "            return 'negative'\n",
    "        elif 0.2 < x:\n",
    "            return 'positive'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda x : check_sentiment(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_sentiment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Tiny evaluation (0.5)\n",
    "Write a function `accuracy` that calculates the accuracy of the sentiment classifier by calculating the fraction of cases where the calculated sentiment matches the original sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(df):\n",
    "    \n",
    "    df['match'] = df['gt_sentiment'] == df['sentiment']\n",
    "    # we could also use mean() here\n",
    "    df['match'].sum()/len(df['match'])\n",
    "    \n",
    "    return df['match'].sum()/len(df['match'])\n",
    "\n",
    "accuracy(df)\n",
    "# output\n",
    "# 0.5791471962616822"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This solution contains word-preprocessing which improves the accuracy to \"0.6810747663551402\"\n",
    "\"\"\"\n",
    "from nltk import word_tokenize , sent_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "puncts = string.punctuation\n",
    "\n",
    "# df = load_dataset()\n",
    "# clean_all_tweets(df)\n",
    "\n",
    "def accuracy2(df):\n",
    "    s_words = stopwords.words('english')\n",
    "    \n",
    "    def word_preprocess(tweet):\n",
    "        text_tokens = word_tokenize(tweet.strip())\n",
    "        # remove the punctuations from the \"text_tokens\"\n",
    "        text_tokens_f = [w for w in text_tokens if w not in puncts]\n",
    "        # remove stopwords\n",
    "        text_tokens_f = [w for w in text_tokens_f if w.lower() not in s_words]\n",
    "        # convert words --> sentence\n",
    "        tweet = ''.join(text_tokens_f)\n",
    "        \n",
    "        sia_polarity = SentimentIntensityAnalyzer().polarity_scores\n",
    "        sentiment = sia_polarity(tweet)['compound']\n",
    "\n",
    "        def check_sentiment(x):\n",
    "            if  x < -.2:\n",
    "                return 'negative'\n",
    "            elif 0.2 < x:\n",
    "                return 'positive'\n",
    "            else:\n",
    "                return 'neutral'\n",
    "            \n",
    "            \n",
    "        sentiment = check_sentiment(sentiment)\n",
    "        return sentiment\n",
    "    \n",
    "    df['sentiment'] = df['cleaned_tweet'].apply(lambda x : word_preprocess(x))\n",
    "            \n",
    "    \n",
    "    df['match'] = df['gt_sentiment'] == df['sentiment']\n",
    "    \n",
    "    # we could also use mean() here\n",
    "    return df['match'].sum()/len(df['match'])\n",
    "\n",
    "accuracy2(df)\n",
    "# output\n",
    "# 0.5791471962616822\n",
    "# this solusiton that we got is \"0.6810747663551402\" accuracy which is 11% more than a naive sentiment with \"0.5791471962616822\"\n",
    "# what we understood is in order to get the better sentiment we have to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Minimum cost spanning-tree (7.0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) Read an example file (1)\n",
    "Consider the data provided in file `'reachability.txt'`. It is consists of a network with nodes representing airports of USA and Canada. Edges are weighted so that there is an edge from city i to city j if the estimated airline travel time is less than a threshold. The travel time includes estimated stopover delays.\n",
    "Write a function `read_graph(filename)` that reads the file at `filename` an returns a directed networkx graph with weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "def read_graph(filename):\n",
    "    \"\"\" we manually counted the number of lines which contain text and skip those\n",
    "    rows but we could also use the below lines of code to find the lines which start \n",
    "    with \"#\" then skip them:\n",
    "    \n",
    "    table = []\n",
    "    filename = 'reachability.txt'\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            table += re.findall('#.+',line)\n",
    "            \n",
    "    skiprows = len(table)\n",
    "    \"\"\"\n",
    "    # \"column_name\" : also we added the names of the columns which is unnecessary \n",
    "    column_name = ['FromNodeId','ToNodeId','Weight']\n",
    "    df = pd.read_table(filename,delimiter=' ',header=None,skiprows=6,names=column_name)\n",
    "\n",
    "    g = nx.DiGraph()\n",
    "    for i in df.itertuples():\n",
    "        g.add_edge(int(i[1]), int(i[2]), weight=-int(i[3]))\n",
    "        \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(27, 0, {'weight': 757}),\n",
       " (27, 1, {'weight': 291}),\n",
       " (27, 2, {'weight': 295}),\n",
       " (27, 3, {'weight': 341}),\n",
       " (27, 4, {'weight': 359})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_large = read_graph('reachability.txt')\n",
    "assert G_large.is_directed()\n",
    "l = list(G_large.edges(data=True))\n",
    "l[:5]\n",
    "# Output\n",
    "# [(27, 0, {'weight': 757}),\n",
    "#  (27, 1, {'weight': 291}),\n",
    "#  (27, 2, {'weight': 295}),\n",
    "#  (27, 3, {'weight': 341}),\n",
    "#  (27, 4, {'weight': 359})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Make the Graph symmetric (1.5)\n",
    "The graph is directed and the weights are asymmetric. Write a function `make_symmetric(G_in)` that takes such a directed graph with weights as input and converts it to an undirected and symmetric network graph. You should use the following simple strategy:\n",
    "\n",
    "If for two nodes u,v , edge u->v has a weight $w_1$ and v->u has weight $w_2$ then symmetric network should have one edge u,v with weight $(w_1 + w_2)/2$. Note that in the final graph each edge should be counted only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, {'weight': 7.5}), (0, 2, {'weight': 3}), (0, 3, {'weight': 1}), (2, 3, {'weight': 15.0}), (3, 4, {'weight': 0.1})]\n"
     ]
    }
   ],
   "source": [
    "def make_symmetric(G_in):\n",
    "    pass\n",
    "\n",
    "# Create example Graph\n",
    "G1 = nx.DiGraph()\n",
    "G1.add_edge(0,1,weight=10)\n",
    "G1.add_edge(1,0,weight=5)\n",
    "G1.add_edge(0,2,weight=3)\n",
    "G1.add_edge(3,0,weight=1)\n",
    "G1.add_edge(2,3,weight=19)\n",
    "G1.add_edge(3,2,weight=11)\n",
    "G1.add_edge(3,4,weight=0.1)\n",
    "G_out = make_symmetric(G1)\n",
    "print(G_out.edges(data=True))\n",
    "# [(0, 1, {'weight': 7.5}), (0, 2, {'weight': 3}), (0, 3, {'weight': 1}), (2, 3, {'weight': 15.0}), (3, 4, {'weight': 0.1})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) Helper class: ConnectedComponentsTracker (2.5)\n",
    "It is sometimes useful to keep track of the connected components https://en.wikipedia.org/wiki/Component_(graph_theory) when building a graph. Therefor write a class `ConnectedComponentsTracker` that keeps track of connected components in its attribute set. This attribute is a list of sets, where if two nodes u,v are in the same sets they are connected, otherwise they are not.\n",
    "It should have these methods:\n",
    "1. The constructor should initialize the `self.sets` to an empty list\n",
    "2. `find_set(self, u)` that returns the set that contains u or `len(self.sets)` if u is not in any of the sets\n",
    "3. `add_edge(self, u, v)` that updates the sets of the connected component to account for the new edge `(u,v)`\n",
    "4. `forms_loop(self, u, v)` that returns True if inserting the undirected edge `(u,v)` would create a loop or is an existing edge, False otherwise\n",
    "\n",
    "The ConnectedComponentsTracker is only intended for use with symmetric Graphs so any edge u,v is can be considered symmetric. You can find example output for a valid `ConnectedComponentsTracker` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [{0, 1}]\n",
      "0.1 False\n",
      "1 [{0, 1, 2}]\n",
      "2 [{0, 1, 2, 3}]\n",
      "2.1 True\n",
      "3 [{0, 1, 2, 3}]\n",
      "4 [{0, 1, 2, 3}, {4, 5}]\n",
      "5 [{0, 1, 2, 3}, {4, 5, 6}]\n",
      "5.1 True\n",
      "5.2 False\n",
      "5.3 False\n",
      "6 [{0, 1, 2, 3, 4, 5, 6}]\n",
      "7 [{0, 1, 2, 3, 4, 5, 6}, {9, 10}]\n",
      "8 [{0, 1, 2, 3, 4, 5, 6, 9, 10}]\n"
     ]
    }
   ],
   "source": [
    "# Test your class here\n",
    "c=ConnectedComponentsTracker()\n",
    "\n",
    "c.add_edge(0,1)\n",
    "print(0, c.sets)\n",
    "print(0.1, c.forms_loop(2,1))\n",
    "c.add_edge(1,2)\n",
    "print(1, c.sets)\n",
    "\n",
    "c.add_edge(3,2)\n",
    "print(2, c.sets)\n",
    "print(2.1, c.forms_loop(1,3))\n",
    "\n",
    "c.add_edge(1,3)\n",
    "print(3, c.sets)\n",
    "\n",
    "c.add_edge(4,5)\n",
    "print(4, c.sets)\n",
    "\n",
    "c.add_edge(5,6)\n",
    "print(5, c.sets)\n",
    "print(5.1, c.forms_loop(4,6))\n",
    "print(5.2, c.forms_loop(1,6))\n",
    "print(5.3, c.forms_loop(1,10))\n",
    "\n",
    "c.add_edge(2,6)\n",
    "print(6, c.sets)\n",
    "\n",
    "c.add_edge(9,10)\n",
    "print(7, c.sets)\n",
    "\n",
    "c.add_edge(10,3)\n",
    "print(8, c.sets)\n",
    "\n",
    "# --- Output ---\n",
    "# Note that the order in a set is not deterministic!\n",
    "# 0   [{0, 1}]\n",
    "# 0.1 False\n",
    "# 1   [{0, 1, 2}]\n",
    "# 2   [{0, 1, 2, 3}]\n",
    "# 2.1 True\n",
    "# 3   [{0, 1, 2, 3}]\n",
    "# 4   [{0, 1, 2, 3}, {4, 5}]\n",
    "# 5   [{0, 1, 2, 3}, {4, 5, 6}]\n",
    "# 5.1 True\n",
    "# 5.2 False\n",
    "# 5.3 False\n",
    "# 6   [{0, 1, 2, 3, 4, 5, 6}]\n",
    "# 7   [{0, 1, 2, 3, 4, 5, 6}, {9, 10}]\n",
    "# 8   [{0, 1, 2, 3, 4, 5, 6, 9, 10}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Kruskal's algorithm (1.5)\n",
    "Obtain minimum cost spanning tree (https://en.wikipedia.org/wiki/Minimum_spanning_tree) using Kruskal's algorithm. The algorithm is given below - \n",
    "\n",
    "1. Sort all the edges in non-decreasing order of their weight.\n",
    "\n",
    "2. Pick the smallest edge (i.e., the edge with the lowest weight). Check if it forms a cycle with the spanning tree formed so far (i.e., all edges selected so far). If cycle is not formed, include this edge. Else, discard it.\n",
    "\n",
    "3. Repeat step#2 until there are (V-1) edges in the spanning tree. \n",
    "\n",
    "V -> number of nodes in the graph\n",
    "\n",
    "Now write a function `minSpanTree` that takes an undirected networkx Graph with weights and returns a new undirected networkx Graph with only those edges that you obtained from Kruskal's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeDataView([(3, 4, {'weight': 0.1}), (3, 0, {'weight': 1}), (0, 2, {'weight': 3}), (0, 1, {'weight': 7.5})])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minSpanTree(G_in):\n",
    "    pass\n",
    "\n",
    "G = minSpanTree(G_out)\n",
    "G.edges(data=True)\n",
    "# [(3, 4, {'weight': 0.1}), (3, 0, {'weight': 1}), (0, 2, {'weight': 3}), (0, 1, {'weight': 7.5})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `minimum_spanning_tree` function of networkx to obtain the minimum cost spanning tree for the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minSpanTreeNetworkx(G):\n",
    "    pass\n",
    "G_netx = minSpanTreeNetworkx(G_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Compare the results (0.25+0.25)\n",
    "Compare your minimal spanning tree with the one from networkx.\n",
    "Therefor write a function `compare_edges(G, G_netx)` that returns the extra and missing edges for the two graphs.\n",
    "Additionally write a function `get_total_span(G)` that returns the total span (sum of all weights) for a Graph.\n",
    "You can use these functions to verify your implementation. Make sure that if there are missing or extra edges that the total span of your implementation is the same as that of networkx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_span(G):\n",
    "    return 0\n",
    "    \n",
    "\n",
    "def compare(G, G_netx):\n",
    "    extra_edges = []\n",
    "    missing_edges = []\n",
    "    # If there is an edge in G but it is not present in G_netx... add to extra_edges\n",
    "    # If there is an edge in G_netx but it is not present in G... add to missing_edges\n",
    "    \n",
    "    return extra_edges, missing_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G_sym = make_symmetric(G_large)\n",
    "G = minSpanTree(G_sym)\n",
    "G_netx = minSpanTreeNetworkx(G_sym)\n",
    "compare(G, G_netx) # There might be some confused edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23864.0\n",
      "23864.0\n"
     ]
    }
   ],
   "source": [
    "# If there are confused edges the total span should be the same still\n",
    "print(get_total_span(G)) \n",
    "print(get_total_span(G_netx))\n",
    "# output\n",
    "# 23864.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
